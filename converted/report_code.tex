% A latex document created by ipypublish
% outline: ipypublish.templates.outline_schemas\latex_outline.latex.j2
% with segments:
% - standard-standard_packages: with standard nbconvert packages
% - standard-standard_definitions: with standard nbconvert definitions
% - ipypublish-doc_article: with the main ipypublish article setup
% - ipypublish-front_pages: with the main ipypublish title and contents page setup
% - ipypublish-biblio_natbib: with the main ipypublish bibliography
% - ipypublish-contents_output: with the main ipypublish content
% - ipypublish-contents_framed_code: with the input code wrapped and framed
% - ipypublish-glossary: with the main ipypublish glossary
%
%%%%%%%%%%%% DOCCLASS

\documentclass[10pt,parskip=half,
toc=sectionentrywithdots,
bibliography=totocnumbered,
captions=tableheading,numbers=noendperiod]{scrartcl}

%%%%%%%%%%%%

%%%%%%%%%%%% PACKAGES

\usepackage[T1]{fontenc} % Nicer default font (+ math font) than Computer Modern for most use cases
\usepackage{mathpazo}
\usepackage{graphicx}
\usepackage[skip=3pt]{caption}
\usepackage{adjustbox} % Used to constrain images to a maximum size
\usepackage[table]{xcolor} % Allow colors to be defined
\usepackage{enumerate} % Needed for markdown enumerations to work
\usepackage{amsmath} % Equations
\usepackage{amssymb} % Equations
\usepackage{textcomp} % defines textquotesingle
% Hack from http://tex.stackexchange.com/a/47451/13684:
\AtBeginDocument{%
    \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
}
\usepackage{upquote} % Upright quotes for verbatim code
\usepackage{eurosym} % defines \euro
\usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
\usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
\usepackage{fancyvrb} % verbatim replacement that allows latex
\usepackage{grffile} % extends the file name processing of package graphics
                        % to support a larger range
% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\usepackage{longtable} % longtable support required by pandoc >1.10
\usepackage{booktabs}  % table support for pandoc > 1.12.2
\usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
\usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                            % normalem makes italics be italics, not underlines

\usepackage{translations}
\usepackage{microtype} % improves the spacing between words and letters
\usepackage{placeins} % placement of figures
% could use \usepackage[section]{placeins} but placing in subsection in command section
% Places the float at precisely the location in the LaTeX code (with H)
\usepackage{float}
\usepackage[colorinlistoftodos,obeyFinal,textwidth=.8in]{todonotes} % to mark to-dos
% number figures, tables and equations by section
% fix for new versions of texlive (see https://tex.stackexchange.com/a/425603/107738)
\let\counterwithout\relax
\let\counterwithin\relax
\usepackage{chngcntr}
% header/footer
\usepackage[footsepline=0.25pt]{scrlayer-scrpage}

% bibliography formatting
\usepackage[numbers, square, super, sort&compress]{natbib}
% hyperlink doi's
\usepackage{doi}

    % define a code float
    \usepackage{newfloat} % to define a new float types
    \DeclareFloatingEnvironment[
        fileext=frm,placement={!ht},
        within=section,name=Code]{codecell}
    \DeclareFloatingEnvironment[
        fileext=frm,placement={!ht},
        within=section,name=Text]{textcell}
    \DeclareFloatingEnvironment[
        fileext=frm,placement={!ht},
        within=section,name=Text]{errorcell}

    \usepackage{listings} % a package for wrapping code in a box
    \usepackage[framemethod=tikz]{mdframed} % to fram code

%%%%%%%%%%%%

%%%%%%%%%%%% DEFINITIONS

% Pygments definitions

\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother

% ANSI colors
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}

% commands and environments needed by pandoc snippets
% extracted from the output of `pandoc -s`
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}

% Additional commands for more recent versions of Pandoc
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}

% Define a nice break command that doesn't care if a line doesn't already
% exist.
\def\br{\hspace*{\fill} \\* }

% Math Jax compatability definitions
\def\gt{>}
\def\lt{<}

\setcounter{secnumdepth}{5}

% Colors for the hyperref package
\definecolor{urlcolor}{rgb}{0,.145,.698}
\definecolor{linkcolor}{rgb}{.71,0.21,0.01}
\definecolor{citecolor}{rgb}{.12,.54,.11}

\DeclareTranslationFallback{Author}{Author}
\DeclareTranslation{Portuges}{Author}{Autor}

\DeclareTranslationFallback{List of Codes}{List of Codes}
\DeclareTranslation{Catalan}{List of Codes}{Llista de Codis}
\DeclareTranslation{Danish}{List of Codes}{Liste over Koder}
\DeclareTranslation{German}{List of Codes}{Liste der Codes}
\DeclareTranslation{Spanish}{List of Codes}{Lista de C\'{o}digos}
\DeclareTranslation{French}{List of Codes}{Liste des Codes}
\DeclareTranslation{Italian}{List of Codes}{Elenco dei Codici}
\DeclareTranslation{Dutch}{List of Codes}{Lijst van Codes}
\DeclareTranslation{Portuges}{List of Codes}{Lista de C\'{o}digos}

\DeclareTranslationFallback{Supervisors}{Supervisors}
\DeclareTranslation{Catalan}{Supervisors}{Supervisors}
\DeclareTranslation{Danish}{Supervisors}{Vejledere}
\DeclareTranslation{German}{Supervisors}{Vorgesetzten}
\DeclareTranslation{Spanish}{Supervisors}{Supervisores}
\DeclareTranslation{French}{Supervisors}{Superviseurs}
\DeclareTranslation{Italian}{Supervisors}{Le autorit\`{a} di vigilanza}
\DeclareTranslation{Dutch}{Supervisors}{supervisors}
\DeclareTranslation{Portuguese}{Supervisors}{Supervisores}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily,
    breakatwhitespace=false,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    breaklines=true,
    literate={\-}{}{0\discretionary{-}{}{-}},
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\lstset{style=mystyle}

\surroundwithmdframed[
  hidealllines=true,
  backgroundcolor=backcolour,
  innerleftmargin=0pt,
  innerrightmargin=0pt,
  innertopmargin=0pt,
  innerbottommargin=0pt]{lstlisting}

%%%%%%%%%%%%

%%%%%%%%%%%% MARGINS

 % Used to adjust the document margins
\usepackage{geometry}
\geometry{tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in,
nohead,includefoot,footskip=25pt}
% you can use showframe option to check the margins visually
%%%%%%%%%%%%

%%%%%%%%%%%% COMMANDS

% ensure new section starts on new page
\addtokomafont{section}{\clearpage}

% Prevent overflowing lines due to hard-to-break entities
\sloppy

% Setup hyperref package
\hypersetup{
    breaklinks=true,  % so long urls are correctly broken across lines
    colorlinks=true,
    urlcolor=urlcolor,
    linkcolor=linkcolor,
    citecolor=citecolor,
    }

% ensure figures are placed within subsections
\makeatletter
\AtBeginDocument{%
    \expandafter\renewcommand\expandafter\subsection\expandafter
    {\expandafter\@fb@secFB\subsection}%
    \newcommand\@fb@secFB{\FloatBarrier
    \gdef\@fb@afterHHook{\@fb@topbarrier \gdef\@fb@afterHHook{}}}%
    \g@addto@macro\@afterheading{\@fb@afterHHook}%
    \gdef\@fb@afterHHook{}%
}
\makeatother

% number figures, tables and equations by section
\counterwithout{figure}{section}
\counterwithout{table}{section}
\counterwithout{equation}{section}
\makeatletter
\@addtoreset{table}{section}
\@addtoreset{figure}{section}
\@addtoreset{equation}{section}
\makeatother
\renewcommand\thetable{\thesection.\arabic{table}}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\theequation{\thesection.\arabic{equation}}

    % set global options for float placement
    \makeatletter
        \providecommand*\setfloatlocations[2]{\@namedef{fps@#1}{#2}}
    \makeatother

% align captions to left (indented)
\captionsetup{justification=raggedright,
singlelinecheck=false,format=hang,labelfont={it,bf}}

% shift footer down so space between separation line
\ModifyLayer[addvoffset=.6ex]{scrheadings.foot.odd}
\ModifyLayer[addvoffset=.6ex]{scrheadings.foot.even}
\ModifyLayer[addvoffset=.6ex]{scrheadings.foot.oneside}
\ModifyLayer[addvoffset=.6ex]{plain.scrheadings.foot.odd}
\ModifyLayer[addvoffset=.6ex]{plain.scrheadings.foot.even}
\ModifyLayer[addvoffset=.6ex]{plain.scrheadings.foot.oneside}
\pagestyle{scrheadings}
\clearscrheadfoot{}
\ifoot{\leftmark}
\renewcommand{\sectionmark}[1]{\markleft{\thesection\ #1}}
\ofoot{\pagemark}
\cfoot{}

%%%%%%%%%%%%

%%%%%%%%%%%% FINAL HEADER MATERIAL

% clereref must be loaded after anything that changes the referencing system
\usepackage{cleveref}
\creflabelformat{equation}{#2#1#3}

% make the code float work with cleverref
\crefname{codecell}{code}{codes}
\Crefname{codecell}{code}{codes}
% make the text float work with cleverref
\crefname{textcell}{text}{texts}
\Crefname{textcell}{text}{texts}
% make the text float work with cleverref
\crefname{errorcell}{error}{errors}
\Crefname{errorcell}{error}{errors}

%%%%%%%%%%%%

\begin{document}

    \begin{titlepage}

  \begin{center}

  \vspace*{1cm}

  \Huge\textbf{Final Project}

  \vspace{0.5cm}\LARGE{Face and Digit Classification}

  \vspace{1.5cm}

  \begin{minipage}{0.8\textwidth}
    \begin{center}
    \begin{minipage}{0.39\textwidth}
    \begin{flushleft} \Large
    \emph{\GetTranslation{Author}:}\\Brian Y. Mahesh A.\\\href{mailto:brianyi14@gmail.com ma1700@scarletmail.rutgers.edu}{brianyi14@gmail.com ma1700@scarletmail.rutgers.edu}
    \end{flushleft}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.39\textwidth}
    \begin{flushright} \Large\emph{\GetTranslation{Supervisors}:} \\
        Abdeslam Boularias
        Aravind S.
    \end{flushright}
    \end{minipage}
    \end{center}
  \end{minipage}

  \vfill

  \begin{minipage}{0.8\textwidth}
  \begin{center}
  \end{center}
  \end{minipage}

  \vspace{0.8cm}
      \LARGE{CS520: Artificial Intelligence}\\
      \LARGE{Rutgers University}\\

  \vspace{0.4cm}

  \today

  \end{center}
  \end{titlepage}

    \begingroup
    \let\cleardoublepage\relax
    \let\clearpage\relax
    \endgroup

\hypertarget{import-and-data-processing}{%
\section{Import and Data Processing}\label{import-and-data-processing}}

For this project, we use NumPy and Pandas modules to import, process,
and transform the raw image data into usable information for model
training. The figue below is an example of the raw digit data (left)
along with the converted image (right) created through our convert()
method; we use the same convert() method to translate the face data as
well. We choose to equate `\#' and `+' pixels with a value of 1 and
empty pixels with a value of 0. We store most of our data in Pandas
Series and DataFrame structures along with Python's native list
structure.

\label{code:example_sym}
\begin{lstlisting}[aboveskip=5pt,basicstyle=\small,belowskip=5pt,breakindent=0pt,language={},numbers=none,postbreak={},xrightmargin=7pt]
array([['                            ', '0000000000000000000000000000'],
       ['                            ', '0000000000000000000000000000'],
       ['                            ', '0000000000000000000000000000'],
       ['                            ', '0000000000000000000000000000'],
       ['                            ', '0000000000000000000000000000'],
       ['                +++++##+    ', '0000000000000000111111110000'],
       ['        +++++######+###+    ', '0000000011111111111111110000'],
       ['       +##########+++++     ', '0000000111111111111111100000'],
       ['        #######+##          ', '0000000011111111110000000000'],
       ['        +++###  ++          ', '0000000011111100110000000000'],
       ['           +#+              ', '0000000000011100000000000000'],
       ['           +#+              ', '0000000000011100000000000000'],
       ['            +#+             ', '0000000000001110000000000000'],
       ['            +##++           ', '0000000000001111100000000000'],
       ['             +###++         ', '0000000000000111111000000000'],
       ['              ++##++        ', '0000000000000011111100000000'],
       ['                +##+        ', '0000000000000000111100000000'],
       ['                 ###+       ', '0000000000000000011110000000'],
       ['              +++###        ', '0000000000000011111100000000'],
       ['            ++#####+        ', '0000000000001111111100000000'],
       ['          ++######+         ', '0000000000111111111000000000'],
       ['        ++######+           ', '0000000011111111100000000000'],
       ['       +######+             ', '0000000111111110000000000000'],
       ['    ++######+               ', '0000111111111000000000000000'],
       ['    +####++                 ', '0000111111100000000000000000'],
       ['                            ', '0000000000000000000000000000'],
       ['                            ', '0000000000000000000000000000'],
       ['                            ', '0000000000000000000000000000']],
      dtype=object)
\end{lstlisting}

\hypertarget{naive-bayes-model}{%
\section{Naive Bayes Model}\label{naive-bayes-model}}

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

For our Naive Bayes model, we predict the classification of an image
through the following Naive Bayes assumption:
\(P(Class|Data) = \frac{P(Data|Class)P(Class)}{P(Data)}\). For each test
observation, we calculate \(P(Class|Data)\) for each possible class and
choose the class with the highest probability as our prediction. When
estimating for \(P(Class|Data)\), we can focus on evaluating
\(P(Data|Class)P(Class)\) and ignore \(P(Data)\) because \(P(Data)\)
remains the same for a given test observation. \(P(Class)\) is the
probability that a certain class appears in a data set and
\(P(Data|Class)\) is the probability that a certain feature value
appears within a certain class. Since we are calculating
\(P(Data|Class)\) through discrete methodology, there may be many values
for a particular feature that do not appear even once in a class; this
causes \(P(Data|Class) = 0\) and messes up our estimation of
\(P(Class|Data)\). To counteract this, we employ a smoothing method over
our model parameters such that any \(P(Data|Class)\) values that equal
zero are assigned a very small value of \(1 \times 10^{-10}\).

\hypertarget{method}{%
\subsection{Method}\label{method}}

We use four methods to train and test our Naive Bayes model:
partition(), feature\_ext(), train\_nb(), and test\_nb(). First consider
the converted digit images that consist of 0s and 1s. Partition()
divides this single image of 0s and 1s into its features and returns
these features as an array. Feature\_ext() takes these arrays of
features returned from partition() and aggregates how often each feature
value appears for each feature. Feature\_ext() returns a dataframe where
the columns represent unique features and the rows represent the total
counts for feature values. From here on, it is a simple matter to
calculate \(P(Data|Class)\) from the dataframe by dividing each feature
value aggregate count by the number of observations in that class. After
calculating \(P(Class)\) and \(P(Data|Class)\) for all unique classes,
we find the class with the highest \(P(Class)P(Data|Class)\) value and
return that as our prediction. We use the same training and testing
method for both digit and face classification since our method is robust
in feature input and adjusts the feature selection algorithm
accordingly.

\hypertarget{digit-classification}{%
\subsection{Digit Classification}\label{digit-classification}}

We first looked at our model's performance on digit classification for
digits 0 through 9. Our feature selection is obtained by dividing a
digit image into n x n dimension partitions such that the sum of colored
pixels (1s) within a partition is considered as one feature. Since each
digit image is 28 x 28 pixels, we initially divided digits into 7 x 7
partitions for a total of 16 features per digit. After we trained this
model using 100\% of the training set, we got an accuracy of 63.8\%.
Being a ways off from the 70\% cutoff, we decided to narrow the size of
each feature to a 4 x 4 partition for a new total of 49 features. By
creating smaller partitions, our features become more precise in
capturing the nuances that differ between each digit. This turned out to
be the better choice and increased our model's overall accuracy from
63.8\% to 74.7\% when trained upon the entire training set.

Table 2.1 shows the accuracy and time it took to train and test our
model using various divisions of our training set over 5 iterations. The
accuracy of our model steadily improved when more of the training set
was used; when our model had more data on various ways to draw each
digit, the model became more accurate when predicting any new test
observations. Note that as more training data was used, the standard
deviation of accuracy measurements decreased over time as well. This
decreased variation in accuracy was the result of using larger training
sets that minimized the chance of any one iteration containing biased
data that skewed the mean accuracy. Overall, the small standard
deviations in accuracy over five testing iterations per training set
told us that our mean accuracy measurements were pretty accurate, which
was reflected in the smoothness of the curve in our Accuracy vs Training
Set graph. With regards to time efficiency, our model took longer to
train when more training data was used; the training times spanned from
3 to 13 seconds when trained on 10\% to 100\% of the training data
respectively.

\begin{table}[H]
\caption{Accuracy and Time of Naive Bayes Digit Classification}\label{tbl:tlabel}
\centering
\begin{adjustbox}{max width=\textwidth}\rowcolors{2}{gray!20}{white}
\begin{tabular}{lrrrrr}
\toprule
{} &  Mean(Accuracy) (\%) &  Std(Accuracy) (\%) &  Training Time (sec) &  Testing Time (sec) &  Total Time (sec) \\
Training &                     &                    &                      &                     &                   \\
\midrule
10\%      &               58.60 &               1.77 &                 2.87 &              111.61 &            114.49 \\
20\%      &               65.12 &               1.85 &                 5.37 &              144.22 &            149.58 \\
30\%      &               69.28 &               0.99 &                 6.34 &              214.24 &            220.58 \\
40\%      &               71.06 &               1.08 &                 9.02 &              224.34 &            233.36 \\
50\%      &               72.58 &               0.98 &                 9.09 &              206.67 &            215.75 \\
60\%      &               73.14 &               0.63 &                10.62 &              267.15 &            277.77 \\
70\%      &               73.94 &               0.69 &                13.26 &              289.50 &            302.75 \\
80\%      &               73.80 &               0.43 &                12.93 &              221.92 &            234.85 \\
90\%      &               74.46 &               0.47 &                11.64 &              224.00 &            235.65 \\
100\%     &               74.70 &               0.00 &                12.70 &              201.94 &            214.64 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\begin{figure}[H]\begin{center}\adjustimage{max size={0.9\linewidth}{0.9\paperheight},height=0.2\paperheight}{report_code_files/output_31_0.png}\end{center}\label{fig:flabel}\end{figure}

\hypertarget{face-classification}{%
\subsection{Face Classification}\label{face-classification}}

For face classification, our goal was to determine whether any
observation was an image of a human face. Our feature selection was
similar to that of digit classification in that we divided a face image
into n x n dimension partitions such that the sum of colored pixels (1s)
within a partition was considered as one feature. Since each face image
was 60 x 70 (width x height) pixels, we divided the image into 2 x 2
partitions for a total of 1050 features. Using 100\% of the training
set, we immediately got an accuracy of 68.7\%. For a Naive Bayes model,
we felt that this accuracy, being 1\% off 70\%, was sufficient in
predicting our face data. Table 2.2 shows the accuracy and time it took
to train and test our model using various divisions of our training set
over five iterations. Like our Naive Bayes model for digit
classification, the accuracy of this model increased while the standard
deviation decreased as more of the training set is used to train our
model. The time it took to train our model also increased as more
training data was used. Overall, our face classification model followed
the same characteristics as the digit model with respect to accuracy and
training time.

\begin{table}[H]
\caption{Accuracy and Time of Naive Bayes Face Classification}\label{tbl:tlabel}
\centering
\begin{adjustbox}{max width=\textwidth}\rowcolors{2}{gray!20}{white}
\begin{tabular}{lrrrrr}
\toprule
{} &  Mean(Accuracy) (\%) &  Std(Accuracy) (\%) &  Training Time (sec) &  Testing Time (sec) &  Total Time (sec) \\
Training &                     &                    &                      &                     &                   \\
\midrule
10\%      &               51.07 &               0.68 &                 4.73 &               27.64 &             32.37 \\
20\%      &               59.20 &               1.29 &                 4.69 &               27.00 &             31.69 \\
30\%      &               63.73 &               1.50 &                 4.97 &               27.22 &             32.18 \\
40\%      &               66.67 &               0.73 &                 4.34 &               23.29 &             27.63 \\
50\%      &               66.93 &               1.37 &                 4.90 &               23.59 &             28.49 \\
60\%      &               67.73 &               2.09 &                 5.92 &               24.27 &             30.19 \\
70\%      &               68.40 &               0.80 &                 6.32 &               23.98 &             30.30 \\
80\%      &               68.53 &               0.65 &                 6.59 &               23.88 &             30.46 \\
90\%      &               68.93 &               0.33 &                 7.01 &               23.83 &             30.84 \\
100\%     &               68.67 &               0.00 &                 7.14 &               23.31 &             30.46 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\begin{figure}[H]\begin{center}\adjustimage{max size={0.9\linewidth}{0.9\paperheight},height=0.2\paperheight}{report_code_files/output_35_0.png}\end{center}\label{fig:flabel}\end{figure}

\hypertarget{perceptron-model}{%
\section{Perceptron Model}\label{perceptron-model}}

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

The perceptron is a single layer neural network and is a classification
algorithm that makes its predictions based on a linear predictor
function that combines a set of weights with a feature vector. For our
perceptron model, we predict the classification of an image using the
following method:

\begin{itemize}
\tightlist
\item
  Extract features from the training dataset and store them in a feature
  set \({\phi}(x)\).
\item
  Initialize function weights \({\{w_j\}}\); note that they may be
  initialized to \(0\).
\item
  For each data point i.e image, \((x_i,y_i)\), in our training dataset:

  \begin{itemize}
  \tightlist
  \item
    For each class label \(y^j\), calculate \(f(x_i,w)\) = \(w_0\) +
    \(w_1{\phi}_1(x_i)\) + \(w_2{\phi}_2(x_i)\) + \(w_3{\phi}_3(x_i)\) +
    . . . + \(w_l{\phi}_l(x_i)\).
  \item
    Select the class label \(y^j\) with the highest \(f(x_i,w)\) value.

    \begin{itemize}
    \tightlist
    \item
      If \(y^j\)=\(y_i\), then nothing needs to be done and we move to
      the next example \((x_{i+1},y_{i+1})\).
    \item
      Else, update the weights \(\{w_{y_i},w_{y^j}\}\):

      \begin{itemize}
      \tightlist
      \item
        For \(y_i\) class label, \(w_{y_i}\) = \(w_{y_i}\) +
        \(\phi(x_i)\)
      \item
        For \(y^j\) class label, \(w_{y^j}\) = \(w_{y^j}\) -
        \(\phi(x_i)\)
      \end{itemize}
    \end{itemize}
  \item
    Repeat this process three more times on the training dataset.
  \end{itemize}
\end{itemize}

After training our model, we return the final weights of each class
label. Using these weights, we calculate the \(f(x_i,w)\) value for each
image in our test dataset and predict the \(y_i\) as the class label
with the highest \(f(x_i,w)\).

\hypertarget{method}{%
\subsection{Method}\label{method}}

We use five methods to train and test our perceptron model: partition(),
feature\_ext(), return\_high\_label(), train\_perceptron(), and
test\_perceptron(). The partition() and feature\_ext() methods are the
same exact methods used in our Naive Bayes model. We have an additional
method, return\_high\_label(), to return the class label with the
highest\(f(x_i,w)\) value. Our train\_perceptron() method trains our
model, returns the trainining time, and computes weights for various
training sets. Note that train\_perceptron() trains our model three
times for one training set because any more training iterations may
cause overfitting and decrease our testing accuracy. With the help of
test\_perceptron() method, we can compute the accuracy of our trained
perceptron model and return the time taken to test the dataset. We use
the same training and testing method for both digit and face
classification since our method is robust in feature input and adjusts
the feature selection algorithm accordingly.

\hypertarget{digit-classification}{%
\subsection{Digit Classification}\label{digit-classification}}

Our feature selection followed the same format as Naive Bayes by
dividing a digit image into n x n dimension partitions such that the sum
of colored pixels (1s) within a partition was considered as one feature.
Since each digit image was 28 x 28, we divided the images into 7 x 7
partitions for a total of 16 features similar to our Naive Bayes
classification. Unfortunately, using 100\% of the training set, we only
got an accuracy of 52.74\%. We decided to narrow the size of each
feature to a 4 x 4 for a total of 49 features and improved our accuracy
to 76.32\%. Finally, we used 1 x 1 partitions for a total of 784 as
features to get an accuracy of 80.40\%. This turned out to be the best
option out of our feature testing and increased our model's overall
accuracy from 52.74\% to 80.40\%.

Table 3.1 shows the accuracy and time it takes to train and test our
model for various divisions of our training set over five iterations. In
the graph below, both training time and accuracy strictly increased with
an increase in the number of training images. There was a slight dip in
accuracy when using 50\% of training data, which can be explained by the
spike in standard deviation of the accuracy measurement when using 50\%
of the training data. After this slight dip, the accuracy continued on
an upwards trend before peaking at 80.40\% when using the 100\% of the
training data. Overall, the standard deviation values were larger than
that of our Naive Bayes models, which was evidenced by a less smooth
curve in the Accuracy vs Training Set graph. With regards to time
efficiency, our model took longer to train when more training data was
used; the training times spanned from 3 to 27 seconds when trained on
10\% to 100\% of the training data respectively.

\begin{table}[H]
\caption{Accuracy and Time of Perceptron Digit Classification}\label{tbl:tlabel}
\centering
\begin{adjustbox}{max width=\textwidth}\rowcolors{2}{gray!20}{white}
\begin{tabular}{lrrrrr}
\toprule
{} &  Mean(Accuracy) (\%) &  Std(Accuracy) (\%) &  Training Time (sec) &  Testing Time (sec) &  Total Time (sec) \\
Training &                     &                    &                      &                     &                   \\
\midrule
10\%      &               68.28 &               3.86 &                 2.63 &                2.14 &              4.77 \\
20\%      &               74.18 &               4.38 &                 5.98 &                2.56 &              8.53 \\
30\%      &               75.96 &               0.79 &                 8.31 &                2.39 &             10.70 \\
40\%      &               78.16 &               1.71 &                11.03 &                2.47 &             13.50 \\
50\%      &               76.30 &               3.76 &                14.20 &                2.42 &             16.62 \\
60\%      &               77.26 &               2.67 &                16.52 &                2.37 &             18.89 \\
70\%      &               77.04 &               1.23 &                19.02 &                2.25 &             21.27 \\
80\%      &               77.66 &               2.04 &                20.73 &                2.29 &             23.02 \\
90\%      &               79.34 &               1.73 &                26.59 &                2.46 &             29.05 \\
100\%     &               80.40 &               1.78 &                26.81 &                2.27 &             29.08 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\begin{figure}[H]\begin{center}\adjustimage{max size={0.9\linewidth}{0.9\paperheight},height=0.2\paperheight}{report_code_files/output_47_0.png}\end{center}\label{fig:flabel}\end{figure}

\hypertarget{face-classification}{%
\subsection{Face Classification}\label{face-classification}}

Our feature selection again divided a face image into n x n dimension
partitions such that the sum of colored pixels (1s) within a partition
was considered as one feature. Since each face image was 60 x 70 (width
x height), we divided the image into 1 x 1 partitions (each pixel as a
feature) for a total of 4200 features; using the full training set, we
got an accuracy of 87.20\%. We experimented with 2 x 2 partitions for a
total of 1050 features and got an increased accuracy of 89.47\% using
the 100\% training dataset.

Table 3.2 shows the accuracy and time it took to train and test our
model for various divisions of our training set over five iterations.
Like our perceptron model for digit classification, the accuracy and
training time increased with the increase in the number of training
images. Again note that the unexpected dip in accuracy at 50\% was a
product of the high standard deviation of 5.10\% for the mean accuracy
for that measurement. After that dip in accuracy, the accuracy increased
much more consistently before reaching the max accuracy at 89.47\%. The
standard deviation values were again larger than that of our Naive Bayes
models, which was evidenced by a less smooth curve in the Accuracy vs
Training Set graph. With regards to time efficiency, our model took
longer to train when more training data was used; the training times
spanned from 0.21 to 1.8 seconds when trained on 10\% to 100\% of the
training data respectively.

\begin{table}[H]
\caption{Accuracy and Time of Perceptron Face Classification}\label{tbl:tlabel}
\centering
\begin{adjustbox}{max width=\textwidth}\rowcolors{2}{gray!20}{white}
\begin{tabular}{lrrrrr}
\toprule
{} &  Mean(Accuracy) (\%) &  Std(Accuracy) (\%) &  Training Time (sec) &  Testing Time (sec) &  Total Time (sec) \\
Training &                     &                    &                      &                     &                   \\
\midrule
10\%      &               66.00 &               6.76 &                 0.21 &                0.39 &              0.60 \\
20\%      &               80.67 &               6.23 &                 0.38 &                0.40 &              0.78 \\
30\%      &               81.33 &               4.15 &                 0.46 &                0.39 &              0.85 \\
40\%      &               86.27 &               2.82 &                 0.60 &                0.38 &              0.98 \\
50\%      &               85.20 &               5.10 &                 0.75 &                0.38 &              1.14 \\
60\%      &               87.47 &               3.19 &                 0.88 &                0.39 &              1.27 \\
70\%      &               88.27 &               2.59 &                 1.12 &                0.40 &              1.52 \\
80\%      &               88.27 &               1.24 &                 1.37 &                0.45 &              1.82 \\
90\%      &               87.60 &               3.64 &                 1.57 &                0.45 &              2.02 \\
100\%     &               89.47 &               2.04 &                 1.80 &                0.46 &              2.25 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\begin{figure}[H]\begin{center}\adjustimage{max size={0.9\linewidth}{0.9\paperheight},height=0.2\paperheight}{report_code_files/output_51_0.png}\end{center}\label{fig:flabel}\end{figure}

\hypertarget{knn-model}{%
\section{KNN Model}\label{knn-model}}

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

For our third model, we chose to use the K-Nearest Neighbor (KNN) model
to predict the classification of an image. KNN is also a form of
supervised learning where the model is trained on a data set with
labels. KNN predicts the class of an observation by finding the K
nearest neighbors of that observation through a designated ``distance''
metric. The distance metric for our model is the sum of the difference
in pixels between two images. Consider two different images of the digit
3 that have been converted into 0s and 1s. We then subtract one matrix
from the other, take the squared value to avoid negative values, and sum
up all the remaining 1s as our final distance value. In the ideal case
where the two images are identical, the calculated distance between the
two images would be zero. Our K nearest neighbors are the K images that
have the smallest distance with respect to our test image. Of these K
nearest neighbors, we select the majority class as our final prediction
for that observation. Usually, an odd number K is selected for a model
with an even number of classes. Since both our digit and image data have
an even number of classes of 10 and 2 respectively, we chose K=7 as our
model parameter.

\hypertarget{method}{%
\subsection{Method}\label{method}}

The methodology for KNN is much simpler than that of our previous
models. Consider one single test observation. If we had to find the K=7
nearest neighbors to this test observation by our distance metric, we
need to find the distance between this test image and every image in the
training set. Since KNN is instance-based learning, our training/testing
procedure is wrapped up in one method that finds the distance between
any test image and all the training images; it then chooses the majority
class of the K images with the smallest distance. Note that since a tie
is much more likely to occur between majority classes in KNN as compared
to our earlier model; our method handles tie breakers through Python's
list's inherent ordering by sorting our K nearest neighbors by distance
and selecting the class in the first index. We use a separate
training/testing method for digit and face classification
(train\_test\_knn and train\_test\_knn2) since there are differences in
feature selection for our two models.

\hypertarget{digit-classification}{%
\subsection{Digit Classification}\label{digit-classification}}

As explained earlier, our features for this model is the distance metric
that is calculated by comparing two images pixel by pixel. We initially
choose k=7 for the number of neighbors when testing our model.
Unfortunately, the model takes a very long time to train and test since
each test observation has to be compared to every single training
observation pixel by pixel; thus, we only took an average accuracy over
three iterations instead of five and used a test set of only 50
observations. Do note that while our training set is randomized each
time for every partition, we chose the same 50 test observations each
time for more consistency.

Table 4.1 shows the accuracy and time it takes to train and test our
model for various divisions of our training set over three iterations.
Figure 4.1 shows two unexpected dips in accuracy when using only 30\%
and 60\% of the training set. This can be explained through the standard
deviation in accuracy measurements between 10-50\% that range between 1
and 2. There must have been iterations where a more extreme accuracy
measurement pulled the mean accuracy away from it's supposed value. Do
note that due to time restrictions, the lack of training/testing
iterations and a smaller testing set of 50 observations plays a major
contribution to this less stable accuracy trend as well. A clear example
of this is the exact same three mean accuracies when using 60-80\% of
the training set due to a combination of high model accuracy and lack of
testing set observations. Our model's peak accuracy when using 100\% of
the training data is 92\%.

Since any calculations are done while testing observations, our KNN
model does not have a ``training time'', and we look at the overall time
efficiency of our model by looking at the training/testing step wrapped
together. One would expect that as more training data is used, there
would be an increase in training/testing time for our model since each
test observation is compared to more training points. Unfortunately,
there doesn't seem to be a clear pattern to our total training/testing
times for our model. Our intuition on the cause of this phenomena is
that the sorting component of our model may vary in that certain testing
images may have a distance array that takes less time to sort in
comparison to others. Overall, the average runtime per iteration is 27
minutes for a total of 13.3 hours over thirty iterations. This long
training/testing time supports our algorithm explanation how KNN needs
to iterate through each training observation for each new test
observation to find the K nearest neighbors.

\begin{table}[H]
\caption{Accuracy and Time of KNN Digit Classification}\label{tbl:tlabel}
\centering
\begin{adjustbox}{max width=\textwidth}\rowcolors{2}{gray!20}{white}
\begin{tabular}{lrrr}
\toprule
{} &  Mean(Accuracy) (\%) &  Std(Accuracy) (\%) &  Time (sec) \\
Training &                     &                    &             \\
\midrule
10\%      &               79.33 &               4.11 &      172.71 \\
20\%      &               87.33 &               1.89 &     1017.01 \\
30\%      &               86.00 &               1.63 &     1574.47 \\
40\%      &               86.67 &               1.89 &     1376.63 \\
50\%      &               88.67 &               2.49 &     1050.52 \\
60\%      &               87.33 &               0.94 &     1526.17 \\
70\%      &               87.33 &               2.49 &     3963.81 \\
80\%      &               87.33 &               1.89 &     2196.54 \\
90\%      &               89.33 &               1.89 &     1514.02 \\
100\%     &               92.00 &               0.00 &     1673.93 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\begin{figure}[H]\begin{center}\adjustimage{max size={0.9\linewidth}{0.9\paperheight},height=0.23\paperheight}{report_code_files/output_62_0.png}\end{center}\caption{Accuracy at Various Training Set Divisions}\label{fig:flabel}\end{figure}

\hypertarget{face-classification}{%
\subsection{Face Classification}\label{face-classification}}

We used the same distance metric that compares two images pixel by pixel
for face classification as well. We chose k=7 for the number of
neighbors for this model due to its efficacy in digit classification. We
once again only took an average over three iterations due to the nature
of KNN training and testing times, although we opted for a larger test
set of size 150 since our training set of 451 data points is
significantly less than the 5000 used in digit classification.
Initially, our model only had an accuracy of 58.67\% when trained upon
the full training set. After looking through the image data again, we
noticed that the deciding features of a face were far more consistent in
the eye, nose, and mouth region. We decided to narrow the focus of our
model to only the middle 25 x 30 pixel region and retrained our model
accordingly. This time, our model's accuracy shot up to 74\% from
58.67\% when trained on the full training set.

Table 4.2 shows the accuracy and time it takes to train and test our
model for various divisions of our training set over three iterations.
Figure 4.2 shows an unexpected decrease in accuracy when using 40\% and
50\% of the training set. This can be explained by taking a look at the
standard deviations of the accuracy measurements between the first
10-30\% training set iterations that tell us whether the mean accuracy
measurement may have been influenced by outlier accuracy iterations.
First, note that the standard deviations during the ``accuracy dip'' are
5.45\% and 4.91\% for 40\% and 50\% (training set partitions)
respectively. Now compare those standard deviations to the standard
deviations measured before the accuracy dip, which are 8.82\%, 9.02\%
and 8.24\%. This allows us to infer that the accuracy measurements
before the accuracy dip may be assumedly higher than it should be and
inflated by an outlier measurement during one of the iterations. The
standard deviation starts to decrease around 70\% as more of the
training set is sampled, leading to a steady increase in accuracy from
there onwards. Our model's peak accuracy when using 100\% of the
training set is 74\%. Overall, the standard deviations range from 5\% to
a shocking 10\% when using 60\% or less of the training set, which can
be explained by the fact that our method only ran three iterations per
training set partition due to runtime restrictions.

With respect to the time efficiency of our model, we again see an uneven
training/testing time as we increase training data partitions. Similar
to digit classification, we believe the sorting component of our
algorithm plays a part to this randomness to the training/testing time
of our KNN model. The average runtime per iteration is 10 minutes for a
total of 5 hours over thirty iterations. This long training/testing time
again supports our algorithm explanation how KNN needs to iterate
through each training observation for each new test observation to find
the K nearest neighbors.

\begin{table}[H]
\caption{Accuracy and Time of KNN Face Classification}\label{tbl:tlabel}
\centering
\begin{adjustbox}{max width=\textwidth}\rowcolors{2}{gray!20}{white}
\begin{tabular}{lrrr}
\toprule
{} &  Mean(Accuracy) (\%) &  Std(Accuracy) (\%) &  Time (sec) \\
Training &                     &                    &             \\
\midrule
10\%      &               56.89 &               8.82 &      161.53 \\
20\%      &               63.11 &               9.02 &      293.37 \\
30\%      &               66.22 &               8.24 &      411.93 \\
40\%      &               64.89 &               5.45 &      631.28 \\
50\%      &               63.11 &               4.91 &      713.95 \\
60\%      &               64.44 &               6.89 &      857.38 \\
70\%      &               67.56 &               3.00 &     1012.93 \\
80\%      &               69.78 &               2.06 &      902.04 \\
90\%      &               72.00 &               1.96 &      434.50 \\
100\%     &               74.00 &               0.00 &      483.65 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\begin{figure}[H]\begin{center}\adjustimage{max size={0.9\linewidth}{0.9\paperheight},height=0.23\paperheight}{report_code_files/output_66_0.png}\end{center}\caption{Accuracy at Various Training Set Divisions}\label{fig:flabel}\end{figure}

\end{document}
